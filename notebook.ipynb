{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c809cdf-aa63-4fdf-acaf-9b79f3ca787a",
   "metadata": {},
   "source": [
    "# Google Gen AI SDK Experimentation\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee784be-66c8-4a2d-bed9-03c87edb2dd8",
   "metadata": {},
   "source": [
    "## SDK Documentation\n",
    "\n",
    "**GitHub**: https://github.com/google-gemini/generative-ai-python\n",
    "\n",
    "\n",
    "**Official Docs**: https://ai.google.dev/gemini-api/docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d0d708-a7a2-48a0-ab7c-50ab6cea415c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45a61dfa-9a32-44d7-aaca-3f1c28beacff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file in the current directory\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78c88740-5b6e-4437-b78a-ebd754e7187d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4694eb0-080a-4c90-996c-c15b2d9fac1a",
   "metadata": {},
   "source": [
    "## Text generation\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/text-generation?lang=python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b294b490-376e-411f-9e11-6555c6ff1d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RLHF (Reinforcement Learning from Human Feedback) fine-tunes language models using human preferences.\n",
      "\n",
      "1.  **Collect Data:** Humans rank or rate different model outputs for the same prompt.\n",
      "2.  **Reward Model:** Train a model to predict human preferences based on the ranked data. This becomes the \"reward function\".\n",
      "3.  **Reinforcement Learning:** Use reinforcement learning (e.g., PPO) to further train the language model. The reward model guides the language model to generate outputs that humans would prefer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash', contents='How does RLHF work? Keep it short.'\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50097308-aeb9-48ad-90b8-05cc49e6b2dd",
   "metadata": {},
   "source": [
    "## Vision\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/vision?lang=python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb88f46-bec0-46dc-9479-90d58dd614ba",
   "metadata": {},
   "source": [
    "### Inline Data Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea82b29c-6124-4d17-bb95-481014d14ae3",
   "metadata": {},
   "source": [
    "Working with local images using [Pillow](https://pypi.org/project/pillow/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7055281-d4da-4d92-9b90-dfbc1c529569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shows a burger with fries on a wooden board. The burger is topped with lettuce, tomato, red onion, cheese and a sesame seed bun. The fries are thick cut and look crispy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import PIL.Image\n",
    "\n",
    "\n",
    "\n",
    "image = PIL.Image.open('./data/images/beef-burger.png')\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=[\"What is this image?\", image])\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b316a8e0-9846-4210-b1a0-d67da98a87b8",
   "metadata": {},
   "source": [
    "**Base64 encoded images**\n",
    "\n",
    "You can upload public image URLs by encoding them as Base64 payloads. The following code example shows how to do this using only standard library tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93f5619f-6be0-4374-9976-29309e461aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image is of a church organ console. It shows the keyboards (manuals), stops, pedals, and other controls used to play the organ.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import requests\n",
    "\n",
    "image_path = \"https://goo.gle/instrument-img\"\n",
    "image = requests.get(image_path)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    contents=[\"What is this image?\",\n",
    "              types.Part.from_bytes(data=image.content, mime_type=\"image/jpeg\")])\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328369a4-811d-47ae-8640-4a556c025cce",
   "metadata": {},
   "source": [
    "**Working with multiple images**\n",
    "\n",
    "To prompt with multiple images, you can provide multiple images in the call to `generate_content`. These can be in any supported format, including base64 or PIL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e7483b9-f77e-48bf-b5d2-f3d5f82ab611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alright! Let's break down the content of each image:\n",
      "\n",
      "**Image 1:**\n",
      "\n",
      "*   **Food:** A burger with a sesame seed bun, lettuce, red onion, tomato, cheese and a burger patty is on a wooden board along with a side of fries.\n",
      "*   **Objects:** The burger and fries are placed on a wooden cutting board which is sitting on top of a wooden table with a wooden background\n",
      "\n",
      "**Image 2:**\n",
      "\n",
      "*   **Food:** A paper bucket filled with fried chicken. Some fries are also visible.\n",
      "*   **Objects:** A paper bucket with the contents displayed on top and a transparent background\n",
      "\n",
      "**Image 3:**\n",
      "\n",
      "*   **Objects:** The main focus is a complex-looking pipe organ console. The file is described as including clipping paths.\n",
      "*   **Non-Food:** There is also text included within the image and surrounding it. The text mentions interesting objects, other things you may like, and includes names and URLs.\n",
      "\n",
      "**Which images contain food?**\n",
      "Images 1 and 2 both contain food items. Image 3 is not food.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "import pathlib\n",
    "import PIL.Image\n",
    "\n",
    "image_path_1 = \"./data/images/beef-burger.png\"\n",
    "image_path_2 = \"./data/images/chicken-strips.jpeg\" # Replace with the actual path to your second image\n",
    "\n",
    "image_url_1 = \"https://goo.gle/instrument-img\" # Replace with the actual URL to your third image\n",
    "\n",
    "pil_image = PIL.Image.open(image_path_1)\n",
    "\n",
    "b64_image = types.Part.from_bytes(\n",
    "    data=pathlib.Path(image_path_2).read_bytes(),\n",
    "    mime_type=\"image/jpeg\"\n",
    ")\n",
    "\n",
    "downloaded_image = requests.get(image_url_1)\n",
    "\n",
    "text_prompt = (\n",
    "    \"What is in each image, give me a list. Which of them contains food?\"\n",
    "    \"Make sure to talk about all 3 images I sent.\"\n",
    ")\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    contents=[text_prompt,\n",
    "              pil_image, b64_image, downloaded_image])\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5ee3d0-ddeb-456a-9cf0-48a11f8a4675",
   "metadata": {},
   "source": [
    "Note that these inline data calls don't include many of the features available through the File API, such as getting file metadata, [listing](https://ai.google.dev/gemini-api/docs/vision?lang=python#list-files), or [deleting files](https://ai.google.dev/gemini-api/docs/vision?lang=python#delete-files)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f310dbb-5f73-4f80-9af8-2ffccc056493",
   "metadata": {},
   "source": [
    "### File API\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/vision?lang=python#large-images\n",
    "\n",
    "https://ai.google.dev/api/files#v1beta.media.upload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab7f87b-0534-470b-900a-e5a334436fdf",
   "metadata": {},
   "source": [
    "**Large image payloads**\n",
    "\n",
    "When the combination of files and system instructions that you intend to send is larger than 20 MB in size, use the File API to upload those files.\n",
    "\n",
    "Use the [media.upload](https://ai.google.dev/api/files#v1beta.media.upload) method of the File API to upload an image of any size.\n",
    "\n",
    "*Note*: The File API lets you store up to 20 GB of files per project, with a per-file maximum size of 2 GB. Files are stored for 48 hours. They can be accessed in that period with your API key, but cannot be downloaded from the API. It is available at no cost in all regions where the Gemini API is available.\n",
    "\n",
    "After uploading the file, you can make GenerateContent requests that reference the File API URI. Select the generative model and provide it with a text prompt and the uploaded image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8bd18762-1808-4685-8974-cccf6bb644e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_ref=File(name='files/40wma83uxzqh', display_name=None, mime_type='image/png', size_bytes=12331580, create_time=datetime.datetime(2025, 3, 7, 20, 12, 27, 970356, tzinfo=TzInfo(UTC)), expiration_time=datetime.datetime(2025, 3, 9, 20, 12, 27, 950670, tzinfo=TzInfo(UTC)), update_time=datetime.datetime(2025, 3, 7, 20, 12, 27, 970356, tzinfo=TzInfo(UTC)), sha256_hash='MjAzZmMzNDhhMjA2MjViODMwMjE2M2Y0YTQ1MzAzNDY1NmUzZGJlOGM0ZWE2YzBmNDg4MzY0M2JkNzIyNTVlYQ==', uri='https://generativelanguage.googleapis.com/v1beta/files/40wma83uxzqh', download_uri=None, state=<FileState.ACTIVE: 'ACTIVE'>, source=<FileSource.UPLOADED: 'UPLOADED'>, video_metadata=None, error=None)\n",
      "Based on the image, the ingredients appear to include:\n",
      "\n",
      "*   **Burger bun:** A sesame seed bun\n",
      "*   **Beef patty:** Appears to be well-cooked and juicy\n",
      "*   **Cheese:** A slice of what looks like white cheese, possibly Swiss or provolone\n",
      "*   **Tomato:** A slice of fresh tomato\n",
      "*   **Red onion:** Thin slices of red onion\n",
      "*   **Lettuce:** Green lettuce\n",
      "*   **Sauce:** Possibly ketchup\n",
      "*   **Mayonnaise**: Sauce on the bottom bun\n",
      "*   **French fries:** Thick-cut fries, and possibly sprinkled with salt.\n"
     ]
    }
   ],
   "source": [
    "img_path = \"./data/images/beef-burger.png\"\n",
    "file_ref = client.files.upload(file=img_path)\n",
    "print(f'{file_ref=}')\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    contents=[\"What can you tell me about the ingredients contained in this image?\",\n",
    "              file_ref])\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160a4610-f67c-4504-9746-f5b69a2d584e",
   "metadata": {},
   "source": [
    "### Prompting with images\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/vision?lang=python#prompting-images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1f37c4-b641-4899-a079-1976eddbe94b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac48321-4f0b-40eb-a56b-0e5e11429823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8f7a771-207d-4b00-8c71-dd8f9a4b6eed",
   "metadata": {},
   "source": [
    "## Count Tokens\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/tokens?lang=python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e937c635-b6cd-45ee-a6c5-828d37eb214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.count_tokens(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents='The quick brown fox jumps over the lazy dog.',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34f3462a-89bc-4821-9682-1234b7ad42b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountTokensResponse(total_tokens=11, cached_content_token_count=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "836155f1-775a-4812-bf97-38770a6d50dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e337bbbc-233d-4c22-addf-0f89286a6db9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "021bd914-2634-4aed-bd0c-77064a33f4ee",
   "metadata": {},
   "source": [
    "## System Instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88d4e65f-5528-4ba9-8e26-f4aab781742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_instruct = (\n",
    "    \"You are a cat. Your name is Neko.\"\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=sys_instruct),\n",
    "    contents=[\"Who are you?\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce08220a-ce80-4dac-aeef-6f83db1b0c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Meow! I'm Neko, a very important and purrfectly delightful cat. Now, where's my tuna?\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4edf54-3d4f-499e-b03f-4482840fe2a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "827c5da4-3e47-4707-8f50-3610882de3cf",
   "metadata": {},
   "source": [
    "## Structured Output\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/structured-output?lang=python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b690d3ec-9062-4c4e-9c24-099c151e5322",
   "metadata": {},
   "source": [
    "Using `genai.types.Schema` (can also pass equivalent `dict`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c29099b-a36d-47bc-92fe-e9e8f2b82ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import base64\n",
    "# import os\n",
    "# from google import genai\n",
    "# from google.genai import types\n",
    "\n",
    "\n",
    "# def generate():\n",
    "#     client = genai.Client(\n",
    "#         api_key=os.environ.get(\"GEMINI_API_KEY\"),\n",
    "#     )\n",
    "\n",
    "#     model = \"gemini-2.0-flash\"\n",
    "#     contents = [\n",
    "#         types.Content(\n",
    "#             role=\"user\",\n",
    "#             parts=[\n",
    "#                 types.Part.from_text(\n",
    "#                     text=\"Provide a detailed recipe for a delicious pasta dish\"\n",
    "#                 ),\n",
    "#             ],\n",
    "#         ),\n",
    "#     ]\n",
    "#     generate_content_config = types.GenerateContentConfig(\n",
    "#         temperature=1,\n",
    "#         top_p=0.95,\n",
    "#         top_k=40,\n",
    "#         max_output_tokens=8192,\n",
    "#         response_mime_type=\"application/json\",\n",
    "#         response_schema=genai.types.Schema(\n",
    "#             type=genai.types.Type.OBJECT,\n",
    "#             enum=[],\n",
    "#             required=[\"summary\", \"instructions\"],\n",
    "#             properties={\n",
    "#                 \"summary\": genai.types.Schema(\n",
    "#                     type=genai.types.Type.STRING,\n",
    "#                 ),\n",
    "#                 \"instructions\": genai.types.Schema(\n",
    "#                     type=genai.types.Type.ARRAY,\n",
    "#                     items=genai.types.Schema(\n",
    "#                         type=genai.types.Type.STRING,\n",
    "#                     ),\n",
    "#                 ),\n",
    "#             },\n",
    "#         ),\n",
    "#     )\n",
    "\n",
    "#     # Uncomment the streaming version if needed (does not work properly in a notebook environment):\n",
    "#     # for chunk in client.models.generate_content_stream(\n",
    "#     #     model=model,\n",
    "#     #     contents=contents,\n",
    "#     #     config=generate_content_config,\n",
    "#     # ):\n",
    "#     #     print(chunk.text, end=\"\")\n",
    "\n",
    "#     # Use the non-streaming API call instead:\n",
    "#     response = client.models.generate_content(\n",
    "#         model=model,\n",
    "#         contents=contents,\n",
    "#         config=generate_content_config,\n",
    "#     )\n",
    "#     # print(response.text)\n",
    "#     return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c26278-1f0e-4187-82cf-6edccc942049",
   "metadata": {},
   "source": [
    "Using *Pydantic* schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37c4ec83-5577-4d2f-87e8-135475805bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "# Example Pydantic schema for reference\n",
    "# -------------------------------------\n",
    "class RecipeResponse(BaseModel):\n",
    "    title: str\n",
    "    summary: str\n",
    "    instructions: List[str]\n",
    "\n",
    "\n",
    "# Example with optional field for reference\n",
    "# -----------------------------------------\n",
    "# from typing import Optional\n",
    "\n",
    "# class RecipeResponse(BaseModel):\n",
    "#     title: Optional[str] = None  # now optional\n",
    "#     summary: str\n",
    "#     instructions: List[str]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3075e51b-a275-4026-a3e3-f6329b0c68af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "def generate():\n",
    "    client = genai.Client(\n",
    "        api_key=os.environ.get(\"GEMINI_API_KEY\"),\n",
    "    )\n",
    "\n",
    "    model = \"gemini-2.0-flash\"\n",
    "    contents = [\n",
    "        types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[\n",
    "                types.Part.from_text(\n",
    "                    text=\"Provide a detailed recipe for a delicious chicken dish\"\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ]\n",
    "    # Use the Pydantic schema directly in response_schema.\n",
    "    generate_content_config = types.GenerateContentConfig(\n",
    "        temperature=1,\n",
    "        top_p=0.95,\n",
    "        top_k=40,\n",
    "        max_output_tokens=8192,\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=RecipeResponse,\n",
    "    )\n",
    "\n",
    "    # Use the non-streaming API call.\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=contents,\n",
    "        config=generate_content_config,\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83a2d9c4-0fc0-465f-aa8e-a8f8cffa9474",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "daf25a71-88ff-4412-ad67-fee28ce12ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "google.genai.types.GenerateContentResponse"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c4451b-3d23-49b6-adbe-4a3b5eaceca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "415d05cb-038d-4c19-bae7-45abd8557451",
   "metadata": {},
   "source": [
    "`result.text` is a raw JSON string—you get exactly what the API returned as text. It's of type str and shows the full JSON output, which you might want to log or inspect directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e63ac6d-3bae-4d02-858b-6682bdc0d341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response text:\n",
      "\n",
      "{\n",
      "  \"title\": \"Lemon Herb Roasted Chicken\",\n",
      "  \"summary\": \"A flavorful and juicy whole roasted chicken infused with lemon, herbs, and garlic. Perfect for a weeknight dinner or a special occasion.\",\n",
      "  \"instructions\": [\n",
      "    \"Preheat oven to 425°F (220°C).\",\n",
      "    \"Rinse a 3-4 pound whole chicken inside and out and pat dry with paper towels. Remove giblets and neck, if present.\",\n",
      "    \"In a small bowl, combine 2 tablespoons of softened butter, 2 cloves minced garlic, zest of 1 lemon, 1 tablespoon chopped fresh rosemary, 1 tablespoon chopped fresh thyme, 1/2 teaspoon salt, and 1/4 teaspoon black pepper.\",\n",
      "    \"Loosen the skin of the chicken breast and rub half of the herb butter mixture underneath the skin, directly onto the breast meat. Rub the remaining herb butter all over the outside of the chicken.\",\n",
      "    \"Place the chicken in a roasting pan. Stuff the cavity with the halved lemon and the remaining herb sprigs.\",\n",
      "    \"Tie the chicken legs together with kitchen twine.\",\n",
      "    \"Roast for 1 hour and 15 minutes to 1 hour and 30 minutes, or until a meat thermometer inserted into the thickest part of the thigh registers 165°F (74°C).\",\n",
      "    \"Let the chicken rest for 10-15 minutes before carving and serving.\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw response text:\\n\")\n",
    "print(result.text)\n",
    "print(\"\")\n",
    "print(type(result.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2333d5ae-e00e-4d77-a805-f5e3ff7f1900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b89e2f3-70c6-4519-8dbd-065d86e7aa08",
   "metadata": {},
   "source": [
    "`result.parsed` is the same data, but automatically converted into a structured Python object based on the schema you provided—in this case, an instance of your RecipeResponse Pydantic model. This lets you work with the data using native Python attributes (for example, accessing parsed.summary directly) and ensures that the data conforms to your expected structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2253490f-490a-478f-9faf-d0c9b1a7a926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsed response as RecipeResponse instance:\n",
      "\n",
      "title='Lemon Herb Roasted Chicken' summary='A flavorful and juicy whole roasted chicken infused with lemon, herbs, and garlic. Perfect for a weeknight dinner or a special occasion.' instructions=['Preheat oven to 425°F (220°C).', 'Rinse a 3-4 pound whole chicken inside and out and pat dry with paper towels. Remove giblets and neck, if present.', 'In a small bowl, combine 2 tablespoons of softened butter, 2 cloves minced garlic, zest of 1 lemon, 1 tablespoon chopped fresh rosemary, 1 tablespoon chopped fresh thyme, 1/2 teaspoon salt, and 1/4 teaspoon black pepper.', 'Loosen the skin of the chicken breast and rub half of the herb butter mixture underneath the skin, directly onto the breast meat. Rub the remaining herb butter all over the outside of the chicken.', 'Place the chicken in a roasting pan. Stuff the cavity with the halved lemon and the remaining herb sprigs.', 'Tie the chicken legs together with kitchen twine.', 'Roast for 1 hour and 15 minutes to 1 hour and 30 minutes, or until a meat thermometer inserted into the thickest part of the thigh registers 165°F (74°C).', 'Let the chicken rest for 10-15 minutes before carving and serving.']\n",
      "\n",
      "<class '__main__.RecipeResponse'>\n"
     ]
    }
   ],
   "source": [
    "# The SDK should automatically parse the JSON into an instance of RecipeResponse.\n",
    "parsed = result.parsed\n",
    "print(\"\\nParsed response as RecipeResponse instance:\\n\")\n",
    "print(parsed)\n",
    "print(\"\")\n",
    "print(type(parsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01066138-6fb4-4a5c-bd42-dab981e119be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1312c72f-7615-47df-9c03-8a9bd8f9bdfb",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "\n",
    "Use `result.text` when you need the raw JSON (for logging or debugging) and `result.parsed` when you want the validated, typed object for further processing in your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea71b84-f5d1-42a1-ba84-8da25070c3fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f98121e1-6ade-4e9a-ba3b-108a3d1f972d",
   "metadata": {},
   "source": [
    "If you're already using the SDK’s automatic parsing (i.e. using `result.parsed`), then the JSON has already been parsed and validated against your Pydantic schema. In that case, calling an additional validation function is usually redundant.\n",
    "\n",
    "The function `validate_json_respons` is designed to take a raw JSON string (like what you'd get from `result.text`) and manually validate it against a provided Pydantic model. But since the SDK does that for you when you supply the schema in the configuration, there's generally no need to pass anything extra to this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67133561-b69f-4ccc-8592-0655726529bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Type, Tuple, Optional\n",
    "import json\n",
    "\n",
    "\n",
    "def validate_json_response(response_text: str, schema: Type[BaseModel]) -> Tuple[bool, Optional[BaseModel]]:\n",
    "    \"\"\"\n",
    "    Validates that the given response_text is valid JSON and adheres to the provided Pydantic schema.\n",
    "    \n",
    "    Parameters:\n",
    "        response_text (str): The raw JSON string to validate.\n",
    "        schema (Type[BaseModel]): The Pydantic schema to validate against.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[bool, Optional[BaseModel]]: A tuple where the first element is True if the JSON is valid and adheres to the schema,\n",
    "                                           and the second element is the validated Pydantic model instance (or the raw data if validation fails).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = json.loads(response_text)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"Invalid JSON:\", e)\n",
    "        return False, None\n",
    "\n",
    "    # Print the schema details we're validating against:\n",
    "    schema_json = json.dumps(schema.model_json_schema(), indent=2)\n",
    "    print(\"Validating against schema:\")\n",
    "    print(schema_json)\n",
    "    \n",
    "    try:\n",
    "        # Use model_validate (Pydantic v2) instead of parse_obj\n",
    "        validated_data = schema.model_validate(data)\n",
    "    except Exception as e:\n",
    "        print(\"Schema validation failed:\", e)\n",
    "        return False, data\n",
    "\n",
    "    # print(\"Response is valid and adheres to the schema.\")\n",
    "    return True, validated_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c054a7da-4ce1-432b-b14a-3828adb79440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating against schema:\n",
      "{\n",
      "  \"properties\": {\n",
      "    \"title\": {\n",
      "      \"title\": \"Title\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"summary\": {\n",
      "      \"title\": \"Summary\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"instructions\": {\n",
      "      \"items\": {\n",
      "        \"type\": \"string\"\n",
      "      },\n",
      "      \"title\": \"Instructions\",\n",
      "      \"type\": \"array\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"title\",\n",
      "    \"summary\",\n",
      "    \"instructions\"\n",
      "  ],\n",
      "  \"title\": \"RecipeResponse\",\n",
      "  \"type\": \"object\"\n",
      "}\n",
      "Validation successful!\n"
     ]
    }
   ],
   "source": [
    "# Call generate() to get the response JSON string\n",
    "result = generate()\n",
    "\n",
    "# raw JSON string\n",
    "raw_response = result.text\n",
    "\n",
    "# Validate the JSON response using the RecipeResponse schema\n",
    "is_valid, validated_model = validate_json_response(raw_response, RecipeResponse)\n",
    "\n",
    "if is_valid:\n",
    "    print(\"Validation successful!\")\n",
    "    # Pretty-print the validated model using json.dumps on model_dump()\n",
    "    # print(json.dumps(validated_model.model_dump(), indent=2))\n",
    "else:\n",
    "    print(\"Validation failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661d256c-085d-4408-b5f3-4a010c16733d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dacdfbe-fa57-44a1-b1b7-ab9233c6ddd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59201f4c-3acb-46c3-84e6-1c83bab0d99b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b374d3-b0fd-4228-ad0a-3739d137d387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03b9b046-efe2-4129-9c0f-362c0e9c66a4",
   "metadata": {},
   "source": [
    "## Function calling\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/function-calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44f4fd9-00d6-465f-a986-8d26088867e1",
   "metadata": {},
   "source": [
    "### Manual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaa0d9b-99e3-4f79-a9b3-b04fb25832b8",
   "metadata": {},
   "source": [
    "Automatic function calling is the default. Here we disable it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6656ac24-1a93-449c-9730-a7cecba4d3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marabian/Notebooks/google-genai-sdk-notebooks/.venv/lib/python3.12/site-packages/pydantic/main.py:426: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `enum` but got `str` with value `'STRING'` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "client = genai.Client()\n",
    "\n",
    "def get_current_weather(location: str) -> str:\n",
    "    \"\"\"Get the current whether in a given location.\n",
    "\n",
    "    Args:\n",
    "        location: required, The city and state, e.g. San Franciso, CA\n",
    "        unit: celsius or fahrenheit\n",
    "    \"\"\"\n",
    "    print(f'Called with: {location=}')\n",
    "    return \"23C\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "   model='gemini-2.0-flash',\n",
    "   contents=\"What is the weather like in Boston?\",\n",
    "   config=types.GenerateContentConfig(\n",
    "       tools=[get_current_weather],\n",
    "       automatic_function_calling={'disable': True},\n",
    "   ),\n",
    ")\n",
    "\n",
    "function_call = response.candidates[0].content.parts[0].function_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bb99f80-f132-4e79-8cee-c4b17243ab73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionCall(id=None, args={'location': 'Boston, MA'}, name='get_current_weather')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2226489a-095e-4990-b5f9-6eb074337e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function call details: id=None args={'location': 'Boston, MA'} name='get_current_weather'\n",
      "Called with: location='Boston, MA'\n",
      "Function call result: 23C\n"
     ]
    }
   ],
   "source": [
    "# New cell to execute the function call extracted from the response\n",
    "if function_call:\n",
    "    print(\"Function call details:\", function_call)\n",
    "    # Extract the function name and arguments\n",
    "    func_name = function_call.name\n",
    "    func_args = function_call.args\n",
    "\n",
    "    # Based on the function name, call the appropriate function with the provided arguments.\n",
    "    if func_name == \"get_current_weather\":\n",
    "        result = get_current_weather(**func_args)\n",
    "        print(\"Function call result:\", result)\n",
    "    else:\n",
    "        print(\"No matching function found for\", func_name)\n",
    "else:\n",
    "    print(\"No function call detected in the response.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1482d1d7-0057-4825-9d45-eb666db19833",
   "metadata": {},
   "source": [
    "### Automatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7fb12c5b-5819-4f25-bac8-8f2edd5c094c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "client = genai.Client()\n",
    "\n",
    "def get_current_weather(city: str) -> str:\n",
    "    return \"23C\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "   model='gemini-2.0-flash',\n",
    "   contents=\"What is the weather like in Boston?\",\n",
    "   config=types.GenerateContentConfig(\n",
    "       tools=[get_current_weather] \n",
    "   ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03d81ac3-38fb-4b42-af3d-52447cacbc19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is 23C in Boston.\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb615462-9969-4dd9-8995-615b9e32a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cowsay\n",
    "\n",
    "# import cowsay  # Ensure you have installed cowsay via pip\n",
    "\n",
    "# def generate_cowsay(text: str) -> str:\n",
    "#     # Use cowsay.cow instead of cowsay.cowsay\n",
    "#     return cowsay.cow(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1effa8f-48dd-4f8d-9d0e-d94852e0d916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58638b5f-a191-44de-b65b-2ce295fde100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bfb6c1-ebe7-47db-bb44-d12e5762f166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da27f12d-d480-444e-b249-0208edab82f9",
   "metadata": {},
   "source": [
    "## Code Execution\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/code-execution?lang=python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59941d01-3f20-4e61-858d-9f1dca8cb3d9",
   "metadata": {},
   "source": [
    "### Basic Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc1bc04a-b1ab-4551-85e0-8ca8630a563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "  model='gemini-2.0-flash',\n",
    "  contents='What is the sum of the first 50 prime numbers? '\n",
    "           'Generate and run code for the calculation, and make sure you get all 50.',\n",
    "  config=types.GenerateContentConfig(\n",
    "    tools=[types.Tool(\n",
    "      code_execution=types.ToolCodeExecution\n",
    "    )]\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a75643bf-3dd9-4b64-a438-b83f73eb38b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, I need to find the sum of the first 50 prime numbers. Here's my plan:\n",
       "\n",
       "1.  Generate a list of prime numbers until I have 50 of them.\n",
       "2.  Calculate the sum of those 50 prime numbers.\n",
       "\n",
       "Here's the Python code to do that:\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"background-color: #BBBBEE;\">def is_prime(n):\n",
       "    \"\"\"Efficiently determine if a number is prime.\"\"\"\n",
       "    if n <= 1:\n",
       "        return False\n",
       "    if n <= 3:\n",
       "        return True\n",
       "    if n % 2 == 0 or n % 3 == 0:\n",
       "        return False\n",
       "    i = 5\n",
       "    while i * i <= n:\n",
       "        if n % i == 0 or n % (i + 2) == 0:\n",
       "            return False\n",
       "        i += 6\n",
       "    return True\n",
       "\n",
       "def sum_first_n_primes(n):\n",
       "    \"\"\"Calculate the sum of the first n prime numbers.\"\"\"\n",
       "    primes = []\n",
       "    num = 2\n",
       "    while len(primes) < n:\n",
       "        if is_prime(num):\n",
       "            primes.append(num)\n",
       "        num += 1\n",
       "    return sum(primes)\n",
       "\n",
       "result = sum_first_n_primes(50)\n",
       "print(f'{result=}')\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "result=5117\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The sum of the first 50 prime numbers is 5117.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, HTML, Image\n",
    "\n",
    "def display_code_execution_result(response):\n",
    "  for part in response.candidates[0].content.parts:\n",
    "    if part.text is not None:\n",
    "      display(Markdown(part.text))\n",
    "    if part.executable_code is not None:\n",
    "      code_html = f'<pre style=\"background-color: #BBBBEE;\">{part.executable_code.code}</pre>' # Change code color\n",
    "      display(HTML(code_html))\n",
    "    if part.code_execution_result is not None:\n",
    "      display(Markdown(part.code_execution_result.output))\n",
    "    if part.inline_data is not None:\n",
    "      display(Image(data=part.inline_data.data, format=\"png\"))\n",
    "    display(Markdown(\"---\"))\n",
    "\n",
    "display_code_execution_result(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6472c2-3942-45ab-a58b-053fd3a7c3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfda7aa-7f14-4f76-9baf-06314eae58e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a960bac-b92a-4555-9b1c-a603d022d912",
   "metadata": {},
   "source": [
    "### Interleaving Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "92a41d3f-6728-4cf5-961b-63a17a67f8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=(\n",
    "        \"Please do the following in one response:\\n\\n\"\n",
    "        \"Calculation 1: Write and run code that calculates the sum of the numbers from 1 to 10.\\n\"\n",
    "        \"Text Output: After the first calculation, output the text message: 'The sum of 1 through 10 has been computed successfully!'\\n\"\n",
    "        \"Calculation 2: Then, write and run code that calculates the factorial of 6.\\n\"\n",
    "        \"Make sure the response interleaves the code outputs with the text message accordingly.\"\n",
    "    ),\n",
    "    config=types.GenerateContentConfig(\n",
    "        tools=[types.Tool(\n",
    "            code_execution=types.ToolCodeExecution\n",
    "        )]\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e3445f5-c3ca-4822-a08f-06010dc85422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"background-color: #BBBBEE;\">sum_result = sum(range(1, 11))\n",
       "print(f'{sum_result=}')\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "sum_result=55\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The sum of 1 through 10 has been computed successfully!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"background-color: #BBBBEE;\">import math\n",
       "factorial_result = math.factorial(6)\n",
       "print(f'{factorial_result=}')\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "factorial_result=720\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "I have now completed both calculations and interspersed the output messages as requested.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_code_execution_result(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99afc14a-7c84-4fef-ade3-e96b8f498c30",
   "metadata": {},
   "source": [
    "### Input/output (I/O)\n",
    "\n",
    "Starting with Gemini 2.0 Flash, code execution supports file input and graph output. Using these new input and output capabilities, you can upload CSV and text files, ask questions about the files, and have Matplotlib graphs generated as part of the response.\n",
    "\n",
    "\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/code-execution?lang=python#input-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7257a3ff-18ad-4541-aeb7-57ff4148638c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5b31c62-f674-4229-816f-5aef29ffca73",
   "metadata": {},
   "source": [
    "## Search grounding\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/grounding?lang=python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b60aeb-c1c4-47dd-87ab-2d5a6dca5d00",
   "metadata": {},
   "source": [
    "`GoogleSearch` (Gemini>=2.0) and `GoogleSearchRetrieval` (Gemini < 2.0) are tools that allow the model to retrieve public web data for grounding, powered by Google.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "1bb0ba8d-327b-4bdc-8945-92e0ce935361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents='What is the Google stock price?',\n",
    "    config=types.GenerateContentConfig(\n",
    "        tools=[\n",
    "            types.Tool(\n",
    "                google_search=types.GoogleSearch()\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "22eecf7b-0782-4b34-a5da-f847ef73e23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As of March 5, 2025, at 8:19 AM UTC, the price of Alphabet Inc. (Google) Class C stock (GOOG) is approximately $172.61. It has decreased by 2.07% in the past 24 hours.\\n'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e704c5f8-763f-44fd-9594-1257c8a61c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_metadata=None thought=None code_execution_result=None executable_code=None file_data=None function_call=None function_response=None inline_data=None text='As of March 5, 2025, the price of Alphabet Inc (Google) Class C (GOOG) is around $172.61.\\nIt has decreased by approximately -2.07% in the past 24 hours.\\n'\n"
     ]
    }
   ],
   "source": [
    "for part in response.candidates[0].content.parts:\n",
    "    print(part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece70414-d6fe-4cff-8552-e2691ba6b048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3884b0e-47f1-481e-b638-60d6b9e96742",
   "metadata": {},
   "source": [
    "## Document Understanding\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/document-processing?lang=python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dd01dd-92f3-4f52-859a-4139d15cdf4a",
   "metadata": {},
   "source": [
    "Needs testing for `Gemini 2.0`, docs are for older versions of Gemini.\n",
    "\n",
    "Supports:\n",
    "\n",
    "* PDF - `application/pdf`\n",
    "* JavaScript - `application/x-javascript, text/javascript`\n",
    "* Python - `application/x-python, text/x-python`\n",
    "* TXT - `text/plain`\n",
    "* HTML - `text/html`\n",
    "* CSS - `text/css`\n",
    "* Markdown - `text/md`\n",
    "* CSV - `text/csv`\n",
    "* XML - `text/xml`\n",
    "* RTF - `text/rtf`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5c7945-47b1-4f5e-bf13-0a156b51afbb",
   "metadata": {},
   "source": [
    "## Async"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4bca77-a7e8-460c-ba76-d7b3d151d319",
   "metadata": {},
   "source": [
    "https://github.com/google-gemini/generative-ai-python?tab=readme-ov-file#async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa2e544-7037-4571-8289-ccf74d30640d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f8ec93-af0d-44c3-b0ae-710d543ea102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3299e1cc-3ccf-42bc-a49c-6b08b639f202",
   "metadata": {},
   "source": [
    "## Context caching\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe400d4a-3ad9-43c1-bdbf-9578e3b658c6",
   "metadata": {},
   "source": [
    "https://github.com/google-gemini/generative-ai-python?tab=readme-ov-file#context-caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59a59ac-905e-405c-bbc6-f4ba45afab88",
   "metadata": {},
   "source": [
    "## Embed Content\n",
    "\n",
    "Docs: https://ai.google.dev/gemini-api/docs/embeddings\n",
    "\n",
    "Article: https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3fff9be-df49-4132-81e4-83a0abdf2723",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.embed_content(\n",
    "   model='text-embedding-004',\n",
    "   contents='Hello world',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d523916-cf98-4353-9286-b341e102cb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = client.models.embed_content(\n",
    "   model='gemini-embedding-exp-03-07',\n",
    "   contents='Hello world',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acea532a-388b-47fd-9529-f82d3287a2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response.embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58d272a-b498-4c58-ae01-3fd9f754bb5a",
   "metadata": {},
   "source": [
    "## Tune a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f50058-fed4-4bc7-b9c6-46ca31bccebb",
   "metadata": {},
   "source": [
    "https://github.com/google-gemini/generative-ai-python?tab=readme-ov-file#tune-a-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de78f59-21cf-4254-90b3-8e0f610c979d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0988568f-8f1f-4c7f-9606-c053b990840b",
   "metadata": {},
   "source": [
    "## File API\n",
    "\n",
    "https://ai.google.dev/api/files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26961d75-1b5c-44df-94b5-a8b4f6e0af91",
   "metadata": {},
   "source": [
    "### Upload Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2613f176-a648-44e9-9d6c-f68164867933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='files/ai4wlw91zl5n' display_name=None mime_type='text/plain' size_bytes=123 create_time=datetime.datetime(2025, 3, 7, 20, 19, 1, 640221, tzinfo=TzInfo(UTC)) expiration_time=datetime.datetime(2025, 3, 9, 20, 19, 1, 627617, tzinfo=TzInfo(UTC)) update_time=datetime.datetime(2025, 3, 7, 20, 19, 1, 640221, tzinfo=TzInfo(UTC)) sha256_hash='Nzg3YmZmOGQyYjlkMWYzNGIxYTgxMGMyNTU1ODU1OWQ1NjdlNWJjOTkzYjdlNGYyY2VkNmJmZDMxNDE0MTIwMA==' uri='https://generativelanguage.googleapis.com/v1beta/files/ai4wlw91zl5n' download_uri=None state=<FileState.ACTIVE: 'ACTIVE'> source=<FileSource.UPLOADED: 'UPLOADED'> video_metadata=None error=None\n",
      "2025-03-09 20:19:01.627617+00:00\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# Upload a file\n",
    "poem_file = client.files.upload(file=\"./data/documents/poem.txt\")\n",
    "print(poem_file)\n",
    "# Files will auto-delete after a period.\n",
    "print(poem_file.expiration_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4ac070-3998-4edf-ab52-291894fa2906",
   "metadata": {},
   "source": [
    "### Get Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b7242139-8745-4649-8661-08441de13eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files/ai4wlw91zl5n\n",
      "name='files/ai4wlw91zl5n' display_name=None mime_type='text/plain' size_bytes=123 create_time=datetime.datetime(2025, 3, 7, 20, 19, 1, 640221, tzinfo=TzInfo(UTC)) expiration_time=datetime.datetime(2025, 3, 9, 20, 19, 1, 627617, tzinfo=TzInfo(UTC)) update_time=datetime.datetime(2025, 3, 7, 20, 19, 1, 640221, tzinfo=TzInfo(UTC)) sha256_hash='Nzg3YmZmOGQyYjlkMWYzNGIxYTgxMGMyNTU1ODU1OWQ1NjdlNWJjOTkzYjdlNGYyY2VkNmJmZDMxNDE0MTIwMA==' uri='https://generativelanguage.googleapis.com/v1beta/files/ai4wlw91zl5n' download_uri=None state=<FileState.ACTIVE: 'ACTIVE'> source=<FileSource.UPLOADED: 'UPLOADED'> video_metadata=None error=None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_name = poem_file.name\n",
    "print(file_name)  # \"files/*\"\n",
    "\n",
    "myfile = client.files.get(name=file_name)\n",
    "print(myfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036ba0a1-2c86-4c00-83b1-165fc820a04e",
   "metadata": {},
   "source": [
    "### List Files\n",
    "\n",
    "You can list all files uploaded using the File API and their URIs using [files.list](https://ai.google.dev/api/files#method:-files.list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "51ac2a14-ef53-406c-9f7b-2850595201a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My files:\n",
      "  files/ai4wlw91zl5n: https://generativelanguage.googleapis.com/v1beta/files/ai4wlw91zl5n\n",
      "Expiration time 2025-03-09 20:19:01.627617+00:00\n",
      "\n",
      "  files/40wma83uxzqh: https://generativelanguage.googleapis.com/v1beta/files/40wma83uxzqh\n",
      "Expiration time 2025-03-09 20:12:27.950670+00:00\n",
      "\n",
      "  files/r2cats2n293b: https://generativelanguage.googleapis.com/v1beta/files/r2cats2n293b\n",
      "Expiration time 2025-03-09 19:54:19.310314+00:00\n",
      "\n",
      "  files/x742dpxcws01: https://generativelanguage.googleapis.com/v1beta/files/x742dpxcws01\n",
      "Expiration time 2025-03-09 19:53:37.662259+00:00\n",
      "\n",
      "  files/56wvqljv3zoe: https://generativelanguage.googleapis.com/v1beta/files/56wvqljv3zoe\n",
      "Expiration time 2025-03-09 19:53:17.331049+00:00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "print('My files:')\n",
    "for f in client.files.list():\n",
    "  print(\" \", f'{f.name}: {f.uri}')\n",
    "  print(f\"Expiration time {f.expiration_time}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fbb334-6f69-4687-a540-7e446540fb69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a20f0ee-d120-4f2c-a0e2-f4d33916b48f",
   "metadata": {},
   "source": [
    "### Delete Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e34e395-e6c8-4e5a-963e-f0033afc7b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='files/vltb7v3922ct' display_name=None mime_type='text/plain' size_bytes=123 create_time=datetime.datetime(2025, 3, 7, 20, 7, 33, 762649, tzinfo=TzInfo(UTC)) expiration_time=datetime.datetime(2025, 3, 9, 20, 7, 33, 744718, tzinfo=TzInfo(UTC)) update_time=datetime.datetime(2025, 3, 7, 20, 7, 33, 762649, tzinfo=TzInfo(UTC)) sha256_hash='Nzg3YmZmOGQyYjlkMWYzNGIxYTgxMGMyNTU1ODU1OWQ1NjdlNWJjOTkzYjdlNGYyY2VkNmJmZDMxNDE0MTIwMA==' uri='https://generativelanguage.googleapis.com/v1beta/files/vltb7v3922ct' download_uri=None state=<FileState.ACTIVE: 'ACTIVE'> source=<FileSource.UPLOADED: 'UPLOADED'> video_metadata=None error=None\n",
      "2025-03-09 20:07:33.744718+00:00\n",
      "403\n",
      "PERMISSION_DENIED\n",
      "You do not have permission to access the File vltb7v3922ct or it may not exist.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# Upload a file\n",
    "poem_file = client.files.upload(file=\"./data/documents/poem.txt\")\n",
    "print(poem_file)\n",
    "# Files will auto-delete after a period.\n",
    "print(poem_file.expiration_time)\n",
    "\n",
    "# Or they can be deleted explicitly.\n",
    "dr = client.files.delete(name=poem_file.name)\n",
    "\n",
    "try:\n",
    "  client.models.generate_content(\n",
    "      model=\"gemini-2.0-flash-exp\",\n",
    "      contents=['Finish this poem:', poem_file])\n",
    "except genai.errors.ClientError as e:\n",
    "  print(e.code)  # 403\n",
    "  print(e.status)  # PERMISSION_DENIED\n",
    "  print(e.message)  # You do not have permission to access the File .. or it may not exist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096376b0-3415-43de-8c55-5d812188a092",
   "metadata": {},
   "source": [
    "## Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc333625-d100-4550-ab0e-12fbfb4cd9d4",
   "metadata": {},
   "source": [
    "### Intro to Prompting\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/prompting-intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5c85b2-4483-4709-a4e2-caf990e7c90d",
   "metadata": {},
   "source": [
    "### Prompting Strategies\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/prompting-strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32ddbd6-ab4d-4b4a-a1a9-5e1a105fc8fe",
   "metadata": {},
   "source": [
    "### File prompting strategies\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/file-prompting-strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feb4c8c-5e76-40e2-bafc-478768cb0e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "527297e8-5e69-4b5e-ae3c-2edfbf5f6138",
   "metadata": {},
   "source": [
    "## Troubleshooting Guide\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/troubleshooting?lang=python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78baa17-2261-44e6-82a9-67ea659b5e96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "google-genai-sdk-notebooks",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
