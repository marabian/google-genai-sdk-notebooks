{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c809cdf-aa63-4fdf-acaf-9b79f3ca787a",
   "metadata": {},
   "source": [
    "# Google Gen AI SDK Experimentation\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee784be-66c8-4a2d-bed9-03c87edb2dd8",
   "metadata": {},
   "source": [
    "## SDK Documentation\n",
    "\n",
    "**GitHub**: https://github.com/google-gemini/generative-ai-python\n",
    "\n",
    "\n",
    "**Official Docs**: https://ai.google.dev/gemini-api/docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d0d708-a7a2-48a0-ab7c-50ab6cea415c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45a61dfa-9a32-44d7-aaca-3f1c28beacff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file in the current directory\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78c88740-5b6e-4437-b78a-ebd754e7187d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4694eb0-080a-4c90-996c-c15b2d9fac1a",
   "metadata": {},
   "source": [
    "## Generate content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b294b490-376e-411f-9e11-6555c6ff1d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RLHF (Reinforcement Learning from Human Feedback) improves AI by:\n",
      "\n",
      "1.  **Training a model:** Train a basic language model.\n",
      "2.  **Collecting human preferences:** Humans rank different model outputs for the same prompt.\n",
      "3.  **Training a reward model:** Use human rankings to train a reward model that predicts human preferences.\n",
      "4.  **Fine-tuning with RL:** Use reinforcement learning to further train the original language model, optimizing it to maximize the reward model's score. This aligns the model with human preferences.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash', contents='How does RLHF work? Keep it short.'\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f7a771-207d-4b00-8c71-dd8f9a4b6eed",
   "metadata": {},
   "source": [
    "## Count Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e937c635-b6cd-45ee-a6c5-828d37eb214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.count_tokens(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents='The quick brown fox jumps over the lazy dog.',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34f3462a-89bc-4821-9682-1234b7ad42b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountTokensResponse(total_tokens=11, cached_content_token_count=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "836155f1-775a-4812-bf97-38770a6d50dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827c5da4-3e47-4707-8f50-3610882de3cf",
   "metadata": {},
   "source": [
    "## Structured Output\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/structured-output?lang=python\n",
    "\n",
    "https://github.com/google-gemini/generative-ai-python?tab=readme-ov-file#json-response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b690d3ec-9062-4c4e-9c24-099c151e5322",
   "metadata": {},
   "source": [
    "Using `genai.types.Schema` (can also pass equivalent `dict`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3c29099b-a36d-47bc-92fe-e9e8f2b82ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import base64\n",
    "# import os\n",
    "# from google import genai\n",
    "# from google.genai import types\n",
    "\n",
    "\n",
    "# def generate():\n",
    "#     client = genai.Client(\n",
    "#         api_key=os.environ.get(\"GEMINI_API_KEY\"),\n",
    "#     )\n",
    "\n",
    "#     model = \"gemini-2.0-flash\"\n",
    "#     contents = [\n",
    "#         types.Content(\n",
    "#             role=\"user\",\n",
    "#             parts=[\n",
    "#                 types.Part.from_text(\n",
    "#                     text=\"Provide a detailed recipe for a delicious pasta dish\"\n",
    "#                 ),\n",
    "#             ],\n",
    "#         ),\n",
    "#     ]\n",
    "#     generate_content_config = types.GenerateContentConfig(\n",
    "#         temperature=1,\n",
    "#         top_p=0.95,\n",
    "#         top_k=40,\n",
    "#         max_output_tokens=8192,\n",
    "#         response_mime_type=\"application/json\",\n",
    "#         response_schema=genai.types.Schema(\n",
    "#             type=genai.types.Type.OBJECT,\n",
    "#             enum=[],\n",
    "#             required=[\"summary\", \"instructions\"],\n",
    "#             properties={\n",
    "#                 \"summary\": genai.types.Schema(\n",
    "#                     type=genai.types.Type.STRING,\n",
    "#                 ),\n",
    "#                 \"instructions\": genai.types.Schema(\n",
    "#                     type=genai.types.Type.ARRAY,\n",
    "#                     items=genai.types.Schema(\n",
    "#                         type=genai.types.Type.STRING,\n",
    "#                     ),\n",
    "#                 ),\n",
    "#             },\n",
    "#         ),\n",
    "#     )\n",
    "\n",
    "#     # Uncomment the streaming version if needed (does not work properly in a notebook environment):\n",
    "#     # for chunk in client.models.generate_content_stream(\n",
    "#     #     model=model,\n",
    "#     #     contents=contents,\n",
    "#     #     config=generate_content_config,\n",
    "#     # ):\n",
    "#     #     print(chunk.text, end=\"\")\n",
    "\n",
    "#     # Use the non-streaming API call instead:\n",
    "#     response = client.models.generate_content(\n",
    "#         model=model,\n",
    "#         contents=contents,\n",
    "#         config=generate_content_config,\n",
    "#     )\n",
    "#     # print(response.text)\n",
    "#     return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c26278-1f0e-4187-82cf-6edccc942049",
   "metadata": {},
   "source": [
    "Using *Pydantic* schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "37c4ec83-5577-4d2f-87e8-135475805bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "# Example Pydantic schema for reference\n",
    "# -------------------------------------\n",
    "class RecipeResponse(BaseModel):\n",
    "    title: str\n",
    "    summary: str\n",
    "    instructions: List[str]\n",
    "\n",
    "\n",
    "# Example with optional field for reference\n",
    "# -----------------------------------------\n",
    "# from typing import Optional\n",
    "\n",
    "# class RecipeResponse(BaseModel):\n",
    "#     title: Optional[str] = None  # now optional\n",
    "#     summary: str\n",
    "#     instructions: List[str]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3075e51b-a275-4026-a3e3-f6329b0c68af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "def generate():\n",
    "    client = genai.Client(\n",
    "        api_key=os.environ.get(\"GEMINI_API_KEY\"),\n",
    "    )\n",
    "\n",
    "    model = \"gemini-2.0-flash\"\n",
    "    contents = [\n",
    "        types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[\n",
    "                types.Part.from_text(\n",
    "                    text=\"Provide a detailed recipe for a delicious chicken dish\"\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ]\n",
    "    # Use the Pydantic schema directly in response_schema.\n",
    "    generate_content_config = types.GenerateContentConfig(\n",
    "        temperature=1,\n",
    "        top_p=0.95,\n",
    "        top_k=40,\n",
    "        max_output_tokens=8192,\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=RecipeResponse,\n",
    "    )\n",
    "\n",
    "    # Use the non-streaming API call.\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=contents,\n",
    "        config=generate_content_config,\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "83a2d9c4-0fc0-465f-aa8e-a8f8cffa9474",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "daf25a71-88ff-4412-ad67-fee28ce12ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "google.genai.types.GenerateContentResponse"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c4451b-3d23-49b6-adbe-4a3b5eaceca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "415d05cb-038d-4c19-bae7-45abd8557451",
   "metadata": {},
   "source": [
    "`result.text` is a raw JSON string—you get exactly what the API returned as text. It's of type str and shows the full JSON output, which you might want to log or inspect directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6e63ac6d-3bae-4d02-858b-6682bdc0d341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response text:\n",
      "\n",
      "{\n",
      "  \"title\": \"Lemon Herb Roasted Chicken\",\n",
      "  \"summary\": \"A flavorful and juicy whole roasted chicken infused with bright lemon and aromatic herbs. Perfect for a family dinner or special occasion.\",\n",
      "  \"instructions\": [\n",
      "    \"Preheat your oven to 425°F (220°C).\",\n",
      "    \"Rinse a 3-4 pound whole chicken inside and out and pat it completely dry with paper towels. Drying the chicken is crucial for crispy skin.\",\n",
      "    \"In a small bowl, combine 2 tablespoons of olive oil, the zest and juice of 1 lemon, 2 cloves of minced garlic, 1 tablespoon of chopped fresh rosemary, 1 tablespoon of chopped fresh thyme, 1 teaspoon of salt, and 1/2 teaspoon of black pepper.\",\n",
      "    \"Loosen the skin of the chicken breast by gently sliding your fingers between the skin and the meat. Be careful not to tear the skin.\",\n",
      "    \"Rub half of the lemon herb mixture under the skin of the chicken breast, distributing it evenly. This will infuse the meat with flavor and help keep it moist.\",\n",
      "    \"Rub the remaining lemon herb mixture all over the outside of the chicken, ensuring it's evenly coated.\",\n",
      "    \"Stuff the cavity of the chicken with the halved lemon (the one you zested and juiced), a quartered onion, and a few sprigs of rosemary and thyme.\",\n",
      "    \"Tie the chicken legs together with kitchen twine to help it cook evenly and maintain its shape.\",\n",
      "    \"Place the chicken in a roasting pan fitted with a roasting rack. If you don't have a rack, you can use roughly chopped carrots, celery, and onion to create a bed for the chicken.\",\n",
      "    \"Roast the chicken for 20 minutes at 425°F (220°C), then reduce the oven temperature to 375°F (190°C) and continue roasting for another 45-60 minutes, or until a meat thermometer inserted into the thickest part of the thigh registers 165°F (74°C).\",\n",
      "    \"Baste the chicken with the pan juices every 20 minutes during the roasting process. This will help keep it moist and flavorful.\",\n",
      "    \"Once the chicken is cooked through, remove it from the oven and let it rest for 10-15 minutes before carving. This allows the juices to redistribute, resulting in a more tender and flavorful bird.\",\n",
      "    \"Carve the chicken and serve with your favorite sides, such as roasted vegetables, mashed potatoes, or a simple salad. Enjoy!\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw response text:\\n\")\n",
    "print(result.text)\n",
    "print(\"\")\n",
    "print(type(result.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2333d5ae-e00e-4d77-a805-f5e3ff7f1900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b89e2f3-70c6-4519-8dbd-065d86e7aa08",
   "metadata": {},
   "source": [
    "`result.parsed` is the same data, but automatically converted into a structured Python object based on the schema you provided—in this case, an instance of your RecipeResponse Pydantic model. This lets you work with the data using native Python attributes (for example, accessing parsed.summary directly) and ensures that the data conforms to your expected structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2253490f-490a-478f-9faf-d0c9b1a7a926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsed response as RecipeResponse instance:\n",
      "\n",
      "title='Lemon Herb Roasted Chicken' summary='A flavorful and juicy whole roasted chicken infused with bright lemon and aromatic herbs. Perfect for a family dinner or special occasion.' instructions=['Preheat your oven to 425°F (220°C).', 'Rinse a 3-4 pound whole chicken inside and out and pat it completely dry with paper towels. Drying the chicken is crucial for crispy skin.', 'In a small bowl, combine 2 tablespoons of olive oil, the zest and juice of 1 lemon, 2 cloves of minced garlic, 1 tablespoon of chopped fresh rosemary, 1 tablespoon of chopped fresh thyme, 1 teaspoon of salt, and 1/2 teaspoon of black pepper.', 'Loosen the skin of the chicken breast by gently sliding your fingers between the skin and the meat. Be careful not to tear the skin.', 'Rub half of the lemon herb mixture under the skin of the chicken breast, distributing it evenly. This will infuse the meat with flavor and help keep it moist.', \"Rub the remaining lemon herb mixture all over the outside of the chicken, ensuring it's evenly coated.\", 'Stuff the cavity of the chicken with the halved lemon (the one you zested and juiced), a quartered onion, and a few sprigs of rosemary and thyme.', 'Tie the chicken legs together with kitchen twine to help it cook evenly and maintain its shape.', \"Place the chicken in a roasting pan fitted with a roasting rack. If you don't have a rack, you can use roughly chopped carrots, celery, and onion to create a bed for the chicken.\", 'Roast the chicken for 20 minutes at 425°F (220°C), then reduce the oven temperature to 375°F (190°C) and continue roasting for another 45-60 minutes, or until a meat thermometer inserted into the thickest part of the thigh registers 165°F (74°C).', 'Baste the chicken with the pan juices every 20 minutes during the roasting process. This will help keep it moist and flavorful.', 'Once the chicken is cooked through, remove it from the oven and let it rest for 10-15 minutes before carving. This allows the juices to redistribute, resulting in a more tender and flavorful bird.', 'Carve the chicken and serve with your favorite sides, such as roasted vegetables, mashed potatoes, or a simple salad. Enjoy!']\n",
      "\n",
      "<class '__main__.RecipeResponse'>\n"
     ]
    }
   ],
   "source": [
    "# The SDK should automatically parse the JSON into an instance of RecipeResponse.\n",
    "parsed = result.parsed\n",
    "print(\"\\nParsed response as RecipeResponse instance:\\n\")\n",
    "print(parsed)\n",
    "print(\"\")\n",
    "print(type(parsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01066138-6fb4-4a5c-bd42-dab981e119be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1312c72f-7615-47df-9c03-8a9bd8f9bdfb",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "\n",
    "Use `result.text` when you need the raw JSON (for logging or debugging) and `result.parsed` when you want the validated, typed object for further processing in your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea71b84-f5d1-42a1-ba84-8da25070c3fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f98121e1-6ade-4e9a-ba3b-108a3d1f972d",
   "metadata": {},
   "source": [
    "If you're already using the SDK’s automatic parsing (i.e. using `result.parsed`), then the JSON has already been parsed and validated against your Pydantic schema. In that case, calling an additional validation function is usually redundant.\n",
    "\n",
    "The function `validate_json_respons` is designed to take a raw JSON string (like what you'd get from `result.text`) and manually validate it against a provided Pydantic model. But since the SDK does that for you when you supply the schema in the configuration, there's generally no need to pass anything extra to this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "67133561-b69f-4ccc-8592-0655726529bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Type, Tuple, Optional\n",
    "import json\n",
    "\n",
    "\n",
    "def validate_json_response(response_text: str, schema: Type[BaseModel]) -> Tuple[bool, Optional[BaseModel]]:\n",
    "    \"\"\"\n",
    "    Validates that the given response_text is valid JSON and adheres to the provided Pydantic schema.\n",
    "    \n",
    "    Parameters:\n",
    "        response_text (str): The raw JSON string to validate.\n",
    "        schema (Type[BaseModel]): The Pydantic schema to validate against.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[bool, Optional[BaseModel]]: A tuple where the first element is True if the JSON is valid and adheres to the schema,\n",
    "                                           and the second element is the validated Pydantic model instance (or the raw data if validation fails).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = json.loads(response_text)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"Invalid JSON:\", e)\n",
    "        return False, None\n",
    "\n",
    "    # Print the schema details we're validating against:\n",
    "    schema_json = json.dumps(schema.model_json_schema(), indent=2)\n",
    "    print(\"Validating against schema:\")\n",
    "    print(schema_json)\n",
    "    \n",
    "    try:\n",
    "        # Use model_validate (Pydantic v2) instead of parse_obj\n",
    "        validated_data = schema.model_validate(data)\n",
    "    except Exception as e:\n",
    "        print(\"Schema validation failed:\", e)\n",
    "        return False, data\n",
    "\n",
    "    # print(\"Response is valid and adheres to the schema.\")\n",
    "    return True, validated_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c054a7da-4ce1-432b-b14a-3828adb79440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating against schema:\n",
      "{\n",
      "  \"properties\": {\n",
      "    \"title\": {\n",
      "      \"title\": \"Title\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"summary\": {\n",
      "      \"title\": \"Summary\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"instructions\": {\n",
      "      \"items\": {\n",
      "        \"type\": \"string\"\n",
      "      },\n",
      "      \"title\": \"Instructions\",\n",
      "      \"type\": \"array\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"title\",\n",
      "    \"summary\",\n",
      "    \"instructions\"\n",
      "  ],\n",
      "  \"title\": \"RecipeResponse\",\n",
      "  \"type\": \"object\"\n",
      "}\n",
      "Validation successful!\n"
     ]
    }
   ],
   "source": [
    "# Call generate() to get the response JSON string\n",
    "result = generate()\n",
    "\n",
    "# raw JSON string\n",
    "raw_response = result.text\n",
    "\n",
    "# Validate the JSON response using the RecipeResponse schema\n",
    "is_valid, validated_model = validate_json_response(raw_response, RecipeResponse)\n",
    "\n",
    "if is_valid:\n",
    "    print(\"Validation successful!\")\n",
    "    # Pretty-print the validated model using json.dumps on model_dump()\n",
    "    # print(json.dumps(validated_model.model_dump(), indent=2))\n",
    "else:\n",
    "    print(\"Validation failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661d256c-085d-4408-b5f3-4a010c16733d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dacdfbe-fa57-44a1-b1b7-ab9233c6ddd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59201f4c-3acb-46c3-84e6-1c83bab0d99b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b374d3-b0fd-4228-ad0a-3739d137d387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03b9b046-efe2-4129-9c0f-362c0e9c66a4",
   "metadata": {},
   "source": [
    "## Function calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44f4fd9-00d6-465f-a986-8d26088867e1",
   "metadata": {},
   "source": [
    "### Manual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaa0d9b-99e3-4f79-a9b3-b04fb25832b8",
   "metadata": {},
   "source": [
    "Automatic function calling is the default. Here we disable it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6656ac24-1a93-449c-9730-a7cecba4d3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "client = genai.Client()\n",
    "\n",
    "def get_current_weather(location: str) -> str:\n",
    "    \"\"\"Get the current whether in a given location.\n",
    "\n",
    "    Args:\n",
    "        location: required, The city and state, e.g. San Franciso, CA\n",
    "        unit: celsius or fahrenheit\n",
    "    \"\"\"\n",
    "    print(f'Called with: {location=}')\n",
    "    return \"23C\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "   model='gemini-2.0-flash',\n",
    "   contents=\"What is the weather like in Boston?\",\n",
    "   config=types.GenerateContentConfig(\n",
    "       tools=[get_current_weather],\n",
    "       automatic_function_calling={'disable': True},\n",
    "   ),\n",
    ")\n",
    "\n",
    "function_call = response.candidates[0].content.parts[0].function_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1bb99f80-f132-4e79-8cee-c4b17243ab73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionCall(id=None, args={'location': 'Boston, MA'}, name='get_current_weather')"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2226489a-095e-4990-b5f9-6eb074337e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function call details: id=None args={'location': 'Boston, MA'} name='get_current_weather'\n",
      "Called with: location='Boston, MA'\n",
      "Function call result: 23C\n"
     ]
    }
   ],
   "source": [
    "# New cell to execute the function call extracted from the response\n",
    "if function_call:\n",
    "    print(\"Function call details:\", function_call)\n",
    "    # Extract the function name and arguments\n",
    "    func_name = function_call.name\n",
    "    func_args = function_call.args\n",
    "\n",
    "    # Based on the function name, call the appropriate function with the provided arguments.\n",
    "    if func_name == \"get_current_weather\":\n",
    "        result = get_current_weather(**func_args)\n",
    "        print(\"Function call result:\", result)\n",
    "    else:\n",
    "        print(\"No matching function found for\", func_name)\n",
    "else:\n",
    "    print(\"No function call detected in the response.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1482d1d7-0057-4825-9d45-eb666db19833",
   "metadata": {},
   "source": [
    "### Automatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7fb12c5b-5819-4f25-bac8-8f2edd5c094c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "client = genai.Client()\n",
    "\n",
    "def get_current_weather(city: str) -> str:\n",
    "    return \"23C\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "   model='gemini-2.0-flash',\n",
    "   contents=\"What is the weather like in Boston?\",\n",
    "   config=types.GenerateContentConfig(\n",
    "       tools=[get_current_weather] \n",
    "   ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "03d81ac3-38fb-4b42-af3d-52447cacbc19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is 23C in Boston.\\n'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "fb615462-9969-4dd9-8995-615b9e32a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cowsay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "55c8f8a2-4168-4296-a954-9ece7209d7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ____________________________\n",
      "| The weather in Boston is 23C |\n",
      "  ============================\n",
      "                            \\\n",
      "                             \\\n",
      "                               ^__^\n",
      "                               (oo)\\_______\n",
      "                               (__)\\       )\\/\\\n",
      "                                   ||----w |\n",
      "                                   ||     ||\n",
      "The weather in Boston is 23C.\n",
      "Unfortunately, I wasn't able to get the cow to say it, but I can tell you that the weather in Boston is 23C.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import cowsay  # Ensure you have installed cowsay via pip\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "def get_current_weather(city: str) -> str:\n",
    "    return \"23C\"\n",
    "\n",
    "def generate_cowsay(text: str) -> str:\n",
    "    # Use cowsay.cow instead of cowsay.cowsay\n",
    "    return cowsay.cow(text)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "   model='gemini-2.0-flash',\n",
    "   contents=\"What is the weather like in Boston and can you show it as a cow saying it?\",\n",
    "   config=types.GenerateContentConfig(\n",
    "       tools=[get_current_weather, generate_cowsay]\n",
    "   ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "c4f338f9-c90a-46b1-8dc0-7abcb8ee5bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in Boston is 23C.\n",
      " moo moo moo\n",
      "/-------------\\\\\n",
      "| The weather in Boston is 23C |\n",
      "\\\\-------------/\n",
      "      ^__^\n",
      "      (oo)\\\\_______\n",
      "      (__)\\\\       )\\\\/\\\\\n",
      "          ||----w |\n",
      "          ||     ||\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1effa8f-48dd-4f8d-9d0e-d94852e0d916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58638b5f-a191-44de-b65b-2ce295fde100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bfb6c1-ebe7-47db-bb44-d12e5762f166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da27f12d-d480-444e-b249-0208edab82f9",
   "metadata": {},
   "source": [
    "## Code Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "484c164d-7652-4ccb-9d59-8c3005b31d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, HTML, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "cc1bc04a-b1ab-4551-85e0-8ca8630a563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "  model='gemini-2.0-flash',\n",
    "  contents='What is the sum of the first 50 prime numbers? '\n",
    "           'Generate and run code for the calculation, and make sure you get all 50.',\n",
    "  config=types.GenerateContentConfig(\n",
    "    tools=[types.Tool(\n",
    "      code_execution=types.ToolCodeExecution\n",
    "    )]\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a75643bf-3dd9-4b64-a438-b83f73eb38b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, I need to find the sum of the first 50 prime numbers. I'll use a Python code block to generate the prime numbers and calculate their sum.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"background-color: #BBBBEE;\">def is_prime(n):\n",
       "  \"\"\"Returns True if n is a prime number, False otherwise.\"\"\"\n",
       "  if n <= 1:\n",
       "    return False\n",
       "  if n <= 3:\n",
       "    return True\n",
       "  if n % 2 == 0 or n % 3 == 0:\n",
       "    return False\n",
       "  i = 5\n",
       "  while i * i <= n:\n",
       "    if n % i == 0 or n % (i + 2) == 0:\n",
       "      return False\n",
       "    i += 6\n",
       "  return True\n",
       "\n",
       "primes = []\n",
       "num = 2\n",
       "while len(primes) < 50:\n",
       "  if is_prime(num):\n",
       "    primes.append(num)\n",
       "  num += 1\n",
       "\n",
       "print(f'{primes=}')\n",
       "print(f'{sum(primes)=}')\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "primes=[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229]\n",
       "sum(primes)=5117\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The first 50 prime numbers are \\[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229]. Their sum is 5117.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_code_execution_result(response):\n",
    "  for part in response.candidates[0].content.parts:\n",
    "    if part.text is not None:\n",
    "      display(Markdown(part.text))\n",
    "    if part.executable_code is not None:\n",
    "      code_html = f'<pre style=\"background-color: #BBBBEE;\">{part.executable_code.code}</pre>' # Change code color\n",
    "      display(HTML(code_html))\n",
    "    if part.code_execution_result is not None:\n",
    "      display(Markdown(part.code_execution_result.output))\n",
    "    if part.inline_data is not None:\n",
    "      display(Image(data=part.inline_data.data, format=\"png\"))\n",
    "    display(Markdown(\"---\"))\n",
    "\n",
    "display_code_execution_result(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99afc14a-7c84-4fef-ade3-e96b8f498c30",
   "metadata": {},
   "source": [
    "### Input/output (I/O)\n",
    "\n",
    "Starting with Gemini 2.0 Flash, code execution supports file input and graph output. Using these new input and output capabilities, you can upload CSV and text files, ask questions about the files, and have Matplotlib graphs generated as part of the response.\n",
    "\n",
    "\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/code-execution?lang=python#input-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7257a3ff-18ad-4541-aeb7-57ff4148638c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5b31c62-f674-4229-816f-5aef29ffca73",
   "metadata": {},
   "source": [
    "## Search grounding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b60aeb-c1c4-47dd-87ab-2d5a6dca5d00",
   "metadata": {},
   "source": [
    "`GoogleSearch` (Gemini>=2.0) and `GoogleSearchRetrieval` (Gemini < 2.0) are tools that allow the model to retrieve public web data for grounding, powered by Google.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "1bb0ba8d-327b-4bdc-8945-92e0ce935361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents='What is the Google stock price?',\n",
    "    config=types.GenerateContentConfig(\n",
    "        tools=[\n",
    "            types.Tool(\n",
    "                google_search=types.GoogleSearch()\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "22eecf7b-0782-4b34-a5da-f847ef73e23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As of March 5, 2025, at 8:19 AM UTC, the price of Alphabet Inc. (Google) Class C stock (GOOG) is approximately $172.61. It has decreased by 2.07% in the past 24 hours.\\n'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e704c5f8-763f-44fd-9594-1257c8a61c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_metadata=None thought=None code_execution_result=None executable_code=None file_data=None function_call=None function_response=None inline_data=None text='As of March 5, 2025, the price of Alphabet Inc (Google) Class C (GOOG) is around $172.61.\\nIt has decreased by approximately -2.07% in the past 24 hours.\\n'\n"
     ]
    }
   ],
   "source": [
    "for part in response.candidates[0].content.parts:\n",
    "    print(part)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5c7945-47b1-4f5e-bf13-0a156b51afbb",
   "metadata": {},
   "source": [
    "## Async"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4bca77-a7e8-460c-ba76-d7b3d151d319",
   "metadata": {},
   "source": [
    "https://github.com/google-gemini/generative-ai-python?tab=readme-ov-file#async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa2e544-7037-4571-8289-ccf74d30640d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f8ec93-af0d-44c3-b0ae-710d543ea102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3299e1cc-3ccf-42bc-a49c-6b08b639f202",
   "metadata": {},
   "source": [
    "## Context caching\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe400d4a-3ad9-43c1-bdbf-9578e3b658c6",
   "metadata": {},
   "source": [
    "https://github.com/google-gemini/generative-ai-python?tab=readme-ov-file#context-caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59a59ac-905e-405c-bbc6-f4ba45afab88",
   "metadata": {},
   "source": [
    "## Embed Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d3fff9be-df49-4132-81e4-83a0abdf2723",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.embed_content(\n",
    "   model='text-embedding-004',\n",
    "   contents='Hello world',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "acea532a-388b-47fd-9529-f82d3287a2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response.embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58d272a-b498-4c58-ae01-3fd9f754bb5a",
   "metadata": {},
   "source": [
    "## Tune a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f50058-fed4-4bc7-b9c6-46ca31bccebb",
   "metadata": {},
   "source": [
    "https://github.com/google-gemini/generative-ai-python?tab=readme-ov-file#tune-a-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de78f59-21cf-4254-90b3-8e0f610c979d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "google-genai-sdk-notebooks",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
